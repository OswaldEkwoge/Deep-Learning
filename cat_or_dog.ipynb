{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "cat_or_dog.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OswaldEkwoge/SD/blob/master/cat_or_dog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-p69ySUz8mJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4MXLCOnhxTw",
        "colab_type": "code",
        "outputId": "6897846b-d0e7-44ac-a2e7-764dcf11bc46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asHdlyeeBp84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb9a8c59-4e5f-4cfd-ad95-b4daa81b53ac"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/Cat or Dog/cats_and_dogs\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat_or_dog.ipynb  dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw1TxAuaz8ne",
        "colab_type": "code",
        "outputId": "066370f0-0698-4d21-8102-cd40297a9ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"Part 1 - Data Preprocessing\"\"\"\n",
        "\n",
        "# Generating images for the Training Set ==> Using the keras documentation for Image Augmentation (using already-ready code)\n",
        "# Type on browser \"keras documentation\" -> Allows us to enrich our dataset without adding more images, allowing us\n",
        "# to get good results with little overfitting\n",
        "# Use the code \"flow from directory method\"\n",
        "\n",
        "# Generating images for the training set\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                  shear_range = 0.2,\n",
        "                                  zoom_range = 0.2,\n",
        "                                  horizontal_flip = True)\n",
        "# Generating images for the test set\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "# Creating the Trainig set\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/Colab Notebooks/Cat or Dog/cats_and_dogs/dataset/training_set',\n",
        "                                                 target_size = (64,64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "# Creating the Test set\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/Colab Notebooks/Cat or Dog/cats_and_dogs/dataset/test_set',\n",
        "                                           target_size = (64,64),\n",
        "                                           batch_size = 32,\n",
        "                                           class_mode = 'binary')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8027 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtSFqiQzz8nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Part 2 - Building the CNN\"\"\"\n",
        "\n",
        "# Initializing the CNN\n",
        "cnn = tf.keras.models.Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "\n",
        "# NB:kernel size is also called feature detector: the \"3\" means a 3x3 matrix, 32 feature detectors will be created\n",
        "# remember we are working with a CPU, so 64 can be tricky.Therefore our CNN will be composed of 32 feature maps\n",
        "# input_shape is the shape of our input images. Since they don't have the same format, we need to force them to have\n",
        "# one single shape (pre-processing phase). So the expected format we choose is 64x64 (number of image dimensions), \n",
        "# the \"3\" is because images are colored.\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[64,64,3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esOGL7uqz8pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2 - Pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34kgPeMfz8pc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding a second convolutional layer (to improve accuracy of results)\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[64,64,3]))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6iupB2wz8qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 3 - Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())   # this results in our input layer on the artificial neural network (64 neurons input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYHIXF5hz8qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 4 - Full Connection\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))   # adding a hidden layer of 128 neurons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMCr7Eklz8rF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 5 - Output layer\n",
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) # 1 output layer, binary (therefore the use of sigmoid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iLBYJTVz8sE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Part 3 - Training the CNN\"\"\"\n",
        "\n",
        "# Compiling the CNN\n",
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc6Jic_zz8sW",
        "colab_type": "code",
        "outputId": "3a48476d-42a6-41a4-847a-672c0f6c2538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        }
      },
      "source": [
        "# Training the CNN on the Training set and evaluating it on the test set\n",
        "cnn.fit_generator(training_set,\n",
        "                 steps_per_epoch = 250,   # this equals number_of_samples / batch_size = 8000 / 32\n",
        "                 epochs = 25,\n",
        "                 validation_data = test_set,\n",
        "                 validation_steps = 62)   # = 2000 / 32"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-25c8c44ed8e9>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/25\n",
            "250/250 [==============================] - 4739s 19s/step - loss: 0.6932 - accuracy: 0.5415 - val_loss: 0.6861 - val_accuracy: 0.5620\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.6200 - accuracy: 0.6569 - val_loss: 0.6027 - val_accuracy: 0.6820\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 43s 172ms/step - loss: 0.5822 - accuracy: 0.6909 - val_loss: 0.5964 - val_accuracy: 0.7016\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 44s 176ms/step - loss: 0.5489 - accuracy: 0.7207 - val_loss: 0.5424 - val_accuracy: 0.7278\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 43s 173ms/step - loss: 0.5258 - accuracy: 0.7425 - val_loss: 0.5418 - val_accuracy: 0.7334\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 43s 173ms/step - loss: 0.5190 - accuracy: 0.7455 - val_loss: 0.5000 - val_accuracy: 0.7641\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 43s 173ms/step - loss: 0.4932 - accuracy: 0.7597 - val_loss: 0.5362 - val_accuracy: 0.7324\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 43s 174ms/step - loss: 0.4797 - accuracy: 0.7677 - val_loss: 0.5088 - val_accuracy: 0.7510\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 43s 173ms/step - loss: 0.4660 - accuracy: 0.7755 - val_loss: 0.5790 - val_accuracy: 0.7364\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 43s 173ms/step - loss: 0.4572 - accuracy: 0.7830 - val_loss: 0.4842 - val_accuracy: 0.7732\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 44s 176ms/step - loss: 0.4505 - accuracy: 0.7822 - val_loss: 0.4908 - val_accuracy: 0.7722\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 43s 172ms/step - loss: 0.4333 - accuracy: 0.7930 - val_loss: 0.5060 - val_accuracy: 0.7681\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.4180 - accuracy: 0.8035 - val_loss: 0.4712 - val_accuracy: 0.7792\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 0.4151 - accuracy: 0.8044 - val_loss: 0.4613 - val_accuracy: 0.7918\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 0.4054 - accuracy: 0.8163 - val_loss: 0.4799 - val_accuracy: 0.7818\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.3951 - accuracy: 0.8220 - val_loss: 0.4644 - val_accuracy: 0.7923\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 43s 173ms/step - loss: 0.3865 - accuracy: 0.8228 - val_loss: 0.5648 - val_accuracy: 0.7515\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 43s 173ms/step - loss: 0.3708 - accuracy: 0.8360 - val_loss: 0.4797 - val_accuracy: 0.7918\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 44s 177ms/step - loss: 0.3613 - accuracy: 0.8371 - val_loss: 0.4851 - val_accuracy: 0.7818\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 43s 174ms/step - loss: 0.3529 - accuracy: 0.8439 - val_loss: 0.4726 - val_accuracy: 0.7954\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 44s 176ms/step - loss: 0.3540 - accuracy: 0.8412 - val_loss: 0.4457 - val_accuracy: 0.8095\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 43s 174ms/step - loss: 0.3467 - accuracy: 0.8442 - val_loss: 0.4951 - val_accuracy: 0.7974\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 0.3370 - accuracy: 0.8444 - val_loss: 0.4619 - val_accuracy: 0.8049\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 43s 174ms/step - loss: 0.3345 - accuracy: 0.8493 - val_loss: 0.4632 - val_accuracy: 0.8095\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 43s 174ms/step - loss: 0.3209 - accuracy: 0.8553 - val_loss: 0.4773 - val_accuracy: 0.7964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fafa8d94a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YscIHI5Hz8wV",
        "colab_type": "code",
        "outputId": "331928b4-42d9-4cb8-a93a-85b2aa6140b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"Given an image, let's predict whether it is a cat or a dog\"\"\"\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_image(img_path, show=False):\n",
        "    img = image.load_img(img_path, target_size=(64, 64))\n",
        "    img_tensor = image.img_to_array(img)\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "    img_tensor /= 255\n",
        "    \n",
        "    if show:\n",
        "        plt.imshow(img_tensor[0])\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    return img_tensor\n",
        "\n",
        "# Choose the image here\n",
        "img_1 = '/content/drive/My Drive/Colab Notebooks/Cat or Dog/cats_and_dogs/dataset/test_image/dog_004.jpg'\n",
        "\n",
        "img_2 = '/content/drive/My Drive/Colab Notebooks/Cat or Dog/cats_and_dogs/dataset/test_image/cat.4010.jpg'\n",
        "\n",
        "# load a single image\n",
        "new_img_1 = load_image(img_1)\n",
        "new_img_2 = load_image(img_2)\n",
        "images = np.vstack([new_img_1, new_img_2])\n",
        "# check prediction\n",
        "pred = cnn.predict(images)\n",
        "print(pred)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.58645725]\n",
            " [0.98779154]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buRKndicz8xA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbE48dYUz8xI",
        "colab_type": "code",
        "outputId": "84c5f080-a035-4f0d-f4fb-a33d7b18bd7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "for  preds in pred:\n",
        "    if preds < 0.5:\n",
        "        print (f\"{preds} -> cat\")\n",
        "    else:\n",
        "        print (f\"{preds} -> dog\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.98635626] -> dog\n",
            "[0.7331336] -> dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctzgr2duz8xU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t-UKjC4z8xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}